---
layout: default
title: Tasks
---

# CLEF 2025 SimpleText: Tasks

---
## How to participate
In order to participate, you should sign up at the [CLEF](https://clef2025.clef-initiative.eu/index.php?page=Pages/registration.html) website: [http://clef2025-labs-registration.dei.unipd.it/](http://clef2024-labs-registration.dei.unipd.it/). 

All team members should join the SimpleText mailing list:
[https://groups.google.com/g/simpletext](https://groups.google.com/g/simpletext). 

The data will be made available to all registered participants.

## Task 1: Text Simplification (simplify scientific text)
This task focuses on simplifying complex scientific texts to improve accessibility, particularly in the biomedical domain. It involves simplifying both sentence-level (Task 1.1) and document-level (Task 1.2) scientific texts. The latest approach for this task includes utilizing a new, extensive corpus created in 2024, specifically tailored to the biomedical field. Additionally, a third sub-task on text alignment is being considered, which requires aligning sentences between two related abstracts (such as source and reference abstracts or source and predictions). This alignment task allows for flexible matching, such as splitting or merging sentences, enhancing the simplification process for complex scientific documents.

### Data
The simplification dataset is built by re-aligning abstracts with their lay summaries on a large scale across sentence, paragraph, and document levels. This corpus is carefully structured to support sentence-level and document-level text simplification, with a special focus on the biomedical domain.

### Evaluation
The evaluation will combine automatic measures (such as SARI, BLEU, LENS, and BERTScore) with human assessments by translation students and professionals, adding a qualitative perspective. CLEF SimpleText has pioneered research in text simplification within information retrieval and NLP, aligning its evaluation approach with related fields like Scholarly Document Processing and the new TREC PABLA track, promoting a standardized methodology across similar tasks.

## Task 2: Identifying and explaining difficult concepts

This task addresses the detection and explanation of complex or "hallucinated" content generated by text simplification systems. Participants are provided with generated texts that may contain creatively exaggerated or unsupported information, with the goal of identifying and mitigating such content. With sources, predictions, and references all closely aligned in the same language, Task 2 explores how to maintain accuracy in generated content, studying how creative or unsupported variations can impact understanding.

### Description

Task 2.1 involves identifying instances of creative generation, particularly in system outputs at the abstract or document level. Participants are provided with realistic outputs from previous years and from known generative models, and they are asked to label sentences that are fully grounded in the source. This includes a focus on labeling sentences with and without access to the source, as well as those introducing significant new content. Task 2.2 shifts to preventing creative generation by ensuring grounding from the start, using source attribution to support claims in the text. Similar to Task 1, it requires participants to submit paired runs—one with and one without source attribution. A related third task, still under discussion, would ask participants to optimally align sentences and paragraphs in both source and output to ensure faithful source attribution.

### Data

Drawing on three years of SimpleText track data, this task uses an extensive collection of predictions and models known for generating spurious or unsupported content. For Task 2.1, data samples highlight unsupported sentences for training, while Task 2.2 adopts a paired run structure similar to that of Task 1, allowing for comparison between source-grounded and creatively generated text pairs.

### Evaluation

Task 2.1 will be assessed with standard classification metrics such as Precision, Recall, and F1 score, with additional token-level evaluations using Jaccard index. For Task 2.2, automatic evaluation measures will be complemented by human assessments, as in Task 1, with the paired structure enabling efficient evaluation of sentence- and phrase-level differences, potentially with tools like MT Unbabel.

## Task 3: SimpleText 2024 Revisited

CLEF 2025 introduces a restructured SimpleText track, aimed at adapting to new objectives and participant interests. Task 4 serves as a transitional track, potentially continuing work from CLEF 2024 tasks based on demand. Specifically, it considers re-running Task 1 on Content Selection (abstract retrieval) and Task 2 on Complexity Spotting (identifying and explaining difficult concepts), and Task 4 on State of the Art (information extraction in scientific documents). The continuation of these tracks is contingent on active interest and input from participants and organizers, with discussions planned at the CLEF 2024 conference in Grenoble.

### Description

CLEF 2025 SimpleText is very different from the earlier years. In order to facilitate the transition to the new track setup, we consider continuing some of the other CLEF 2024 SimpleText tasks (Task 1 on Content Selection: abstract retrieval, Task 2 on Complexity Spotting: identifying and explaining difficult concepts, Task 4 on SotA: tracking the state-of-the-art in scholarly publications). We will only continue those activities by request of, and with sufficient interest from, our active participants. 

### Data and evaluation

For further reference on methodology and evaluation criteria, details are available in the LNCS track overview paper by Ermakova et al. (2024b), as well as in the CEUR task overview papers for CLEF 2024 SimpleText Task 1 (Sanjuan et al., 2024), Task 2 (Di Nunzio et al., 2024), and Task 4 (D’Souza et al., 2024).  We aim to set up leaderboards for these tasks at Codalabs.
